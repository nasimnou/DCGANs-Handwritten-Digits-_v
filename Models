######Generator##############

class generator_A(nn.Module):

    def __init__(self):
   
        super(generator_A, self).__init__()
  
       
        self.fc1 = nn.Linear(z_dim, 7 * 7 * 256)
        ###################reshape in forward 

        # Transposed convolution layer, from 7x7x256 into 14x14x128 tensor
        self.conv1 = nn.ConvTranspose2d(256, 128,3,stride=2,padding=1,output_padding=1)
         #nn.ConvTranspose2d(  )
        # Batch normalization
        self.BN1 = nn.BatchNorm2d(128)

        ###################Leaky ReLU

        # Transposed convolution layer, from 14x14x128 to 14x14x64 tensor
        self.conv2 = nn.ConvTranspose2d(128,64, 3,stride=1, padding=1,output_padding=0)
         # Batch normalization
        self.BN2 = nn.BatchNorm2d(64)

        ###################Leaky ReLU

        # Transposed convolution layer, from 14x14x64 to 28x28x1 tensor
        self.conv3 = nn.ConvTranspose2d(64,1, 3,stride=2,padding=1,output_padding=1)

        ###################TAN activation 

    def forward(self,x):
      y=self.fc1(x)
      y = y.view(-1,256,7,7)
      y = F.leaky_relu(self.BN1(self.conv1(y)),negative_slope=0.2)
      y = F.leaky_relu(self.BN2(self.conv2(y)),negative_slope=0.2)
      y = F.tanh(self.conv3(y))

      
      return y
      
      
    ######Discriminator##############
    
    class discriminator(nn.Module):

    def __init__(self):
      super(discriminator, self).__init__()
      
      #Convolutional layer, from 28x28x1 into 14x14x32 tensor
      self.conv1 = nn.Conv2d(1, 32,3,stride=2, padding=1)
      ################### Leaky ReLU

      # Convolutional layer, from 14x14x32 into 7x7x64 tensor
      self.conv2 = nn.Conv2d(32, 64,3,stride=2, padding=1)

      # Batch normalization
      self.BN1 = nn.BatchNorm2d(64)

      ################### Leaky ReLU

      # Convolutional layer, from 7x7x64 tensor into 3x3x128 tensor
      self.conv3 = nn.Conv2d(64, 128,3,stride=2, padding=0)

      # Batch normalization
      self.BN2 = nn.BatchNorm2d(128)

      ################### Leaky ReLU

      ###################Reshape

      # Dense layer,from 128*3*3 to channel 1
      self.fc1 = nn.Linear(1152,1)



        
    def forward(self,x):
      y = F.leaky_relu(self.conv1(x),negative_slope=0.2)
      y = F.leaky_relu(self.BN1(self.conv2(y)),negative_slope=0.2)
      y = F.leaky_relu(self.BN2(self.conv3(y)),negative_slope=0.2)


      y = y.view(y.size()[0],-1)
      y= F.sigmoid(self.fc1(y))

      return y



  


